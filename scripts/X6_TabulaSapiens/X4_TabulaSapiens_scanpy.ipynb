{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f79ad1",
   "metadata": {},
   "source": [
    "# Runs Cytocipher using scanpy scores on the full Tabula Sapiens data.\n",
    "                     \n",
    "# INPUT: \n",
    "\n",
    "    * data/test_data/TabulaSapiens_Cytociphered.h5ad\n",
    "\n",
    "# OUTPUT: \n",
    "\n",
    "    * data/test_data/scanpy_tabula-full_results.pkl\n",
    "\n",
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce9f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = './' # Change to where cloned Cytocipher_manuscript/\n",
    "import os\n",
    "os.chdir(work_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.visualisation.helpers as vhs\n",
    "import utils.visualisation.quick_plots as qpl\n",
    "qpl.setUp()\n",
    "\n",
    "import seaborn as sb\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "pre_dir = '/media/WorkingSpace/Share/hypo_atlas/'\n",
    "data_dir = pre_dir+'data/test_data/'\n",
    "out_dir = data_dir\n",
    "\n",
    "import cytocipher as cc\n",
    "import importlib as imp\n",
    "\n",
    "import cytocipher.score_and_merge.cluster_merge as cm\n",
    "import cytocipher.score_and_merge.cluster_score as cs\n",
    "imp.reload(cm)\n",
    "imp.reload(cs)\n",
    "\n",
    "import utils.preprocessing.load_data.simple_pickle as spl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14971277",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9148337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'scanpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea312588",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.read_h5ad(data_dir+'TabulaSapiens_Cytociphered.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5041f7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(483152, 58870)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5f29c",
   "metadata": {},
   "source": [
    "## Now let's run using Code-scoring to get p-values.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f443653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def get_process_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27688104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanpy_merge(data, cluster_key, marker_key,\n",
    "                 k=15, mnn_frac_cutoff=None, random_state=20, p_cut=0.01, score_group_method='quantiles',\n",
    "                 p_adjust=True, p_adjust_method='fdr_bh'):\n",
    "    \"\"\" Extra marker calling in order to be in-line with the cc.tl.merge_clusters function.\n",
    "    \"\"\"\n",
    "    cc.tl.get_markers(data, cluster_key, var_groups='highly_variable')\n",
    "    \n",
    "    ### Scanpy-scoring.\n",
    "    scanpy_scores = np.zeros((data.shape[0], len(data.uns[marker_key])))\n",
    "    for i, group in enumerate(data.uns[marker_key]):\n",
    "        sc.tl.score_genes(data, data.uns[marker_key][group])\n",
    "        scanpy_scores[:,i] = data.obs['score'].values\n",
    "    scanpy_scores = pd.DataFrame(scanpy_scores, index=data.obs_names, columns = list(data.uns[marker_key].keys()))\n",
    "    data.obsm[f'{cluster_key}_enrich_scores'] = scanpy_scores\n",
    "    \n",
    "    ### Merging by Scanpy-score.\n",
    "    cm.merge_clusters_single(data, cluster_key, f'{cluster_key}_merged',\n",
    "                            k=k, mnn_frac_cutoff=mnn_frac_cutoff, random_state=random_state,\n",
    "                          p_cut=p_cut,\n",
    "                          score_group_method=score_group_method,\n",
    "                          p_adjust=p_adjust, p_adjust_method=p_adjust_method,\n",
    "                          verbose=False)\n",
    "    \n",
    "    cc.tl.get_markers(data, f'{cluster_key}_merged', var_groups='highly_variable')\n",
    "    \n",
    "    ### Scanpy-scoring Final round for merge.\n",
    "    scanpy_scores = np.zeros((data.shape[0], len(data.uns[f'{cluster_key}_merged_markers'])))\n",
    "    for i, group in enumerate(data.uns[f'{cluster_key}_merged_markers']):\n",
    "        sc.tl.score_genes(data, data.uns[f'{cluster_key}_merged_markers'][group])\n",
    "        scanpy_scores[:,i] = data.obs['score'].values\n",
    "    scanpy_scores = pd.DataFrame(scanpy_scores, index=data.obs_names, \n",
    "                                 columns = list(data.uns[f'{cluster_key}_merged_markers'].keys()))\n",
    "    data.obsm[f'{cluster_key}_merged_enrich_scores'] = scanpy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b016248",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now re-running with the coexpr scoring...\n",
    "import time\n",
    "import tracemalloc\n",
    "start = time.time()\n",
    "start_mem = get_process_memory()\n",
    "tracemalloc.start()\n",
    "# cc.tl.merge_clusters(data, 'overclusters', var_groups='highly_variable', n_cpus=15, max_iter=0,\n",
    "#                      enrich_method='code', \n",
    "#                       k=None, squash_exception=True,\n",
    "#                      )\n",
    "scanpy_merge(data, 'overclusters', 'overclusters_markers', k=None)\n",
    "file_ = open(data_dir+f'tabula_{method}-full-mem.txt', 'w')\n",
    "print(tracemalloc.get_traced_memory(), file=file_)\n",
    "tracemalloc.stop()\n",
    "end = time.time()\n",
    "end_mem = get_process_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83a51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lapse = end-start\n",
    "print(\"Time in seconds: \", lapse, file=file_)\n",
    "lapse_mem = end_mem - start_mem\n",
    "print(\"Memory used: \", lapse_mem, file=file_)\n",
    "file_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c31dd8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4345451145, 15774847579)\r\n",
      "Time in seconds:  13662.974824905396\r\n",
      "Memory used:  4156055552\r\n"
     ]
    }
   ],
   "source": [
    "!cat {data_dir}tabula_{method}-full-mem.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f7ea60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.429396434"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15774847579-4345451145)/1e9 #### Memory in Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1813941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7952707846959433"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13662.974824905396/60/60 #### Time in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cacheing results from Coexpr-scoring\n",
    "enrich = method\n",
    "results = {'enrich': enrich}\n",
    "results[f'overclusters_{enrich}_scores'] = data.obsm[f'overclusters_enrich_scores']\n",
    "results[f'overclusters_merged_{enrich}_scores'] = data.obsm[f'overclusters_merged_enrich_scores']\n",
    "results[f'overclusters_{enrich}_merged'] = data.obs['overclusters_merged']\n",
    "uns_keys = [key for key in data.uns.keys() if key.startswith('overclusters_')]\n",
    "for key in uns_keys:\n",
    "    results[f'{key}_{enrich}'] = data.uns[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3814f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.pl.enrich_heatmap(data, 'overclusters', per_cell=False, scale_cols=False, scale_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving these results as a pickle object\n",
    "spl.saveAsPickle(data_dir+f'{enrich}_tabula-full_results.pkl', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
